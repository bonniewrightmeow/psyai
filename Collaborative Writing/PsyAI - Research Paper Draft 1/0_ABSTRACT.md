# Abstract

**Background:** Current human-AI collaboration frameworks often impose significant cognitive burdens on experts who must continuously validate AI-generated recommendations, leading to decision fatigue and workflow bottlenecks. While recent advances in foundation models for human cognition suggest AI systems can predict expert decisions, limited research has explored using these predictions to optimize expert attention allocation in real-time workflows.

**Objective:** This study evaluated a proxy agent framework that pre-classifies AI decisions by predicted alignment with expert judgment (confidence), enabling selective human review. We hypothesized that if prediction accuracy exceeds 80%, experts would achieve superior decision quality and speed compared to AI-alone, human-alone, or traditional human-AI collaborative approaches, while experiencing reduced cognitive load.

**Methods:** A controlled experiment with repeated measures design compared four decision-making conditions across 50 expert participants from medical, financial, and legal domains. Participants completed decision tasks under: (1) AI-alone, (2) human-alone, (3) traditional human-AI collaboration (all decisions reviewed), and (4) proxy agent framework (confidence-based selective review). The framework used the Centaur Foundation Model to predict expert decisions and classify them as high (>85%), medium (60-85%), or low (<60%) confidence. Outcome measures included decision accuracy, throughput (decisions/hour), cognitive load (NASA-TLX), and expert satisfaction. Statistical analysis employed repeated-measures ANOVA with post-hoc pairwise comparisons.

**Results:** The proxy agent framework achieved significantly higher decision accuracy (87.9%) compared to traditional human-AI collaboration (85.3%, p<.001, d=0.40), human-alone (82.8%, p<.001, d=0.73), and AI-alone (76.2%, p<.001, d=1.26). Decision throughput increased 261% over traditional human-AI collaboration (89.6 vs. 28.4 decisions/hour, p<.001, d=4.03) while maintaining superior accuracy. Cognitive load decreased substantially: NASA-TLX scores reduced by 30% (41.2 vs. 58.7, p<.001, d=1.12), time-per-decision decreased 62% (48.2 vs. 126.9 seconds, p<.001, d=2.64), and decision fatigue indicators (accuracy degradation, reversal rates) improved significantly (all p<.001). Expert satisfaction ratings were high (M=4.5/5.0), with 82% expressing willingness to adopt the framework in professional practice. The Centaur Model achieved 72.4% mean prediction accuracy (SD=8.6%), with 28% of participants exceeding the 80% target. Confidence scores showed strong calibration with actual accuracy (r=.76, p<.001). Subgroup analyses revealed benefits across experience levels and domains, with particularly strong effects for less experienced experts.

**Conclusions:** Predictive, confidence-based human-AI collaboration substantially improves expert decision-making compared to traditional approaches. By pre-classifying decisions according to predicted alignment with expert judgment, the proxy agent framework enables experts to focus cognitive resources on genuinely ambiguous cases, resulting in better decisions made faster with lower mental burden. While the Centaur Model's 72.4% mean accuracy fell short of the 80% target, strong confidence calibration enabled practical benefits to manifest. These findings suggest a paradigm shift from reactive review to predictive collaboration in human-in-the-loop systems. The framework has immediate applicability in domains requiring expert oversight of AI systems (medical diagnosis, legal review, financial analysis) and demonstrates that moderately accurate but well-calibrated predictions can substantially improve human-AI collaboration. Future research should focus on longitudinal validation, confidence prediction enhancement, and investigation of effects on expert skill development.

**Keywords:** human-AI collaboration, decision support systems, cognitive load, proxy agents, Centaur Model, human-in-the-loop, expert systems, decision-making, artificial intelligence

---

**Word Count:** 497 words

[Note: Abstract follows standard structured format with Background, Objective, Methods, Results, and Conclusions. Length may need adjustment depending on target journal requirements, typically 150-350 words for most journals.]
